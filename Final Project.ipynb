{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e131dce-2501-4480-b969-1e78462eb9c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c19896-a9eb-4ffa-ba63-048bd8857ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration step\n",
    "MAX_VOCAB = 10000        # I put to limit the vocab size to only the first 10,000 words \n",
    "MAX_LEN = 100            # pads all the sequences to 100 tokens\n",
    "EMBED_DIM = 64           # Embeds the output size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "# All of the emotion labels that are used for application to each text\n",
    "LABELS = ['admiration', 'amusement', 'gratitude', 'love', 'pride', 'relief', 'remorse']\n",
    "\n",
    "# Function used to clean input text and remove any special characters \n",
    "def clean_text(text):\n",
    "    text = text.lower()  # converts all text to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # removes any URLs within the text inputs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # removes any mentions like seen in discord\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \"\", text)  # remove emojis/special chars\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26861b43-ad7e-4296-b621-d5841b06862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the training CSV and cleans the text within each input\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df['text'] = train_df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Tokenizes the text and prepares the input features\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df['text'].tolist())\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'].tolist())\n",
    "X_train = pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Extracts the label matrix for multi-hot encoding\n",
    "y_train = train_df[LABELS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b92e3d9-bd5d-4bc5-b6d8-ec6fa865730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the sequential Bidirectional Long-Short Term Memory model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB, output_dim=EMBED_DIM),\n",
    "    Bidirectional(LSTM(64)),  # I input it as Bidirectional for better sequence understanding within the context\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(LABELS), activation='sigmoid')  # Sigmoid for interpreting outputs as probabilities\n",
    "])\n",
    "\n",
    "# Compile the model using binary crossentropy \n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "773a7554-fb2d-48b5-86db-d53fddace557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 45ms/step - accuracy: 0.5844 - loss: 0.2250 - val_accuracy: 0.6401 - val_loss: 0.1223\n",
      "Epoch 2/5\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - accuracy: 0.6018 - loss: 0.0883 - val_accuracy: 0.8063 - val_loss: 0.0810\n",
      "Epoch 3/5\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 44ms/step - accuracy: 0.6846 - loss: 0.0606 - val_accuracy: 0.7075 - val_loss: 0.0799\n",
      "Epoch 4/5\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.6762 - loss: 0.0498 - val_accuracy: 0.5254 - val_loss: 0.0858\n",
      "Epoch 5/5\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.6702 - loss: 0.0391 - val_accuracy: 0.6429 - val_loss: 0.0967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x175ccf89910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defined an early stopping to prevent overfitting since I noticed it was overfitting significantly at higher epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model using a 90/10 split where 90% is training and 10% is validation\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e4696f5-adda-4c62-a6a4-f883c5fc479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
      "Dev Set Micro F1 Score: 0.8135\n",
      "Dev Set Macro F1 Score: 0.5966\n"
     ]
    }
   ],
   "source": [
    "#This portion is to check the F1 scores of the model through the dev.csv dataset\n",
    "try:\n",
    "    # Loads and cleans the development set\n",
    "    dev_df = pd.read_csv(\"dev.csv\")\n",
    "    dev_df['text'] = dev_df['text'].astype(str).apply(clean_text)\n",
    "    \n",
    "    # Prepares the dev inputs\n",
    "    dev_sequences = tokenizer.texts_to_sequences(dev_df['text'].tolist())\n",
    "    X_dev = pad_sequences(dev_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    y_dev = dev_df[LABELS].values\n",
    "\n",
    "    # Predicts the probabilities and threshold at 0.5\n",
    "    dev_preds = model.predict(X_dev)\n",
    "    dev_binary_preds = (dev_preds > 0.5).astype(int)\n",
    "\n",
    "    # Provides the calculations of the micro and macro F1 scores\n",
    "    micro_f1 = f1_score(y_dev, dev_binary_preds, average='micro')\n",
    "    macro_f1 = f1_score(y_dev, dev_binary_preds, average='macro')\n",
    "\n",
    "    print(f\"Dev Set Micro F1 Score: {micro_f1:.4f}\")\n",
    "    print(f\"Dev Set Macro F1 Score: {macro_f1:.4f}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"dev.csv not found. Skipping evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51493241-094d-4cf8-8525-3b2c4f0809bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"
     ]
    }
   ],
   "source": [
    "# Loads and cleans the test set\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df['text'] = test_df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Tokenizes and pads the test data\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'].tolist())\n",
    "X_test = pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicts the probabilities and converts them to binary labels\n",
    "test_preds = model.predict(X_test)\n",
    "test_binary_preds = (test_preds > 0.5).astype(int)\n",
    "\n",
    "# Prepares the final submission dataframe\n",
    "submission_df = pd.DataFrame(test_binary_preds, columns=LABELS)\n",
    "submission_df.insert(0, \"text\", test_df[\"text\"])\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cb7bde1-1d1b-45d2-bf96-2cfe3b031404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved as 'submission.zip'\n"
     ]
    }
   ],
   "source": [
    "# Zips the CSV into submission.zip for proper formatting to submit to codabench\n",
    "with zipfile.ZipFile(\"submission.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(\"submission.csv\")\n",
    "\n",
    "print(\"Submission saved as 'submission.zip'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b37a6-9d12-4741-a069-1fee8d024d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
